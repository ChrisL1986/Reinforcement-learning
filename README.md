# Overview
This repository provides implementations for some examples presented in the book "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto.


# Table of Contents
[Gambler's-Problem](#gamblers)
[Black Jack](#blackjack)
[Random Walk](#randomwalk)
[Short Corridor](#shortcorridor)


<a name="gamblers"/>
# Gambler's-Problem
Optimal policy for the gamblers problem using dynamic progarmming (reinforcement learning) as presented in  example 4.3 in "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto.

Value estimates for different probabilities:

![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/value_p0.25.png)
![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/value_p0.4.png)
![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/value_p0.55.png)

Optimal policy for different probabilities:
![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/policy_p0.25.png)
![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/policy_p0.4.png)
![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/policy_p0.55.png)


<a name="blackjack"/>
# Black Jack
Optimal value and policy for BlackJack using the Monte Carlo method with exploring states as presented in  example 5.1 in "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto. 

![alt text](https://github.com/ChrisL1986/Reinforcement-learning/blob/images/policy.png)


<a name="randomwalk"/>
# Random Walk
Implementation of different strategies for the example 6.2  in "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto. 


<a name="shortcorridor"/>
# Short corridor with switched actions
Implementation of "REINFORCE: Monte Carlo Policy Gradient" and an action-value method with Îµ-greedy action selection as presented in example 13.1 in "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto. 

